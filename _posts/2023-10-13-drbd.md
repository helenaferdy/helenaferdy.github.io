---
title: DRBD (Distributed Replicated Block Device)
date: 2023-10-13 11:30:00 +0700
categories: [Storage, DRBD]
tags: [DRBD]
---

<br>

## What is Distributed Replicated Block Device?

DRBD (Distributed Replicated Block Device) is a Linux software solution for replicating data between servers. It ensures data consistency and is often used in high-availability clusters to minimize downtime in case of hardware or software failures.

![x](/static/2023-10-13-drbd/00.png)

<br>
<br>

## Preparing Disk Partition

On the SDA Physical Disk, we have 100 GB of free space, we'll create a new partition named sda4 with the size of 50 GB for DRBD

![x](/static/2023-10-13-drbd/01.png)

<br>

To do that, run "fdisk /dev/sda"

> The command "fdisk /dev/sda" is used to interact with the disk partitioning utility called fdisk on the /dev/sda device. This command allows us to create, modify, and manage disk partitions on the first SCSI or SATA hard drive (sda) in the system.

<br>

type 'n' to create new partition

```shell
Welcome to fdisk (util-linux 2.37.2).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

This disk is currently in use - repartitioning is probably a bad idea.
It's recommended to umount all file systems, and swapoff all swap
partitions on this disk.


Command (m for help): n
```

<br>

Select partition number

```shell
Command (m for help): n
Partition number (4-128, default 4): 4
```

<br>

Select default for first and last sector

<br>

select 't' to change partition ID, then select 4, and for partition alias type '8e'. <br>
The hex code ‘8e’ is the code for a Linux LVM which is what we want this partition to be, as we will be joining it with the original /dev/sda4 Linux LVM

```shell
Command (m for help): t
Partition number (1-4, default 4): 4
Partition type or alias (type L to list all): 8e

Type of partition 4 is unchanged: Linux filesystem.

Command (m for help): w
```

Lastly type 'w' to write the changes

![x](/static/2023-10-13-drbd/02.png)

<br>

Run 'lsblk' to see the newly created partition

![x](/static/2023-10-13-drbd/03.png)

<br>

Do the same for the second node

![x](/static/2023-10-13-drbd/03a.png)

<br>
<br>

## Configuring DRBD

Install the DRBD

```shell
sudo apt update
sudo apt install drbd-utils -y
```

<br>

Then create the DRBD configuration

```shell
sudo nano /etc/drbd.conf
```

<br>

Set the synchronization type here

```conf
common {
    protocol C;
}
```

> * Protocol A (Async): Allows the primary node to acknowledge writes before they are synchronized with the secondary node. Offers better performance but may have some data consistency delay.
> * Protocol B (Semi-Sync): Acknowledges writes only after they are confirmed on both primary and secondary nodes, striking a balance between data consistency and performance.
> * Protocol C (Sync): Ensures strict data consistency by acknowledging writes only after they have been written to both the primary and secondary nodes. Provides the highest level of data integrity but may impact performance.

<br>

Then configure the nodes member

```conf
resource r0 {
    on storage1 {
        device /dev/drbd0;
        disk /dev/sda4;
        address 198.18.0.31:7788;
        meta-disk internal;
    }
    on storage2 {
        device /dev/drbd0;
        disk /dev/sda4;
        address 198.18.0.32:7788;
        meta-disk internal;
    }
}
```

<br>

Here's what we end up with

![x](/static/2023-10-13-drbd/05.png)

<br>

Next create the metadata for a DRBD resource named "r0"

```shell
drbdadm create-md r0
```

![x](/static/2023-10-13-drbd/06.png)

<br>

Then start the DRBD service

```shell
systemctl start drbd
```

<br>

Now if we run "lsblk", we can see the drbd0 device created

![x](/static/2023-10-13-drbd/07.png)

<br>

Run "cat /proc/drbd" to see the DRBD status

![x](/static/2023-10-13-drbd/08.png)

<br>

Repeat the same process for the second node until we have the same state

![x](/static/2023-10-13-drbd/08a.png)

<br>

As we can see here, both nodes are in the secondary state.
Run this command on node1 to initiate the sync and make it the primary node

```shell
drbdadm -- --overwrite-data-of-peer primary r0/0
```

<br>

Running "cat /proc/drbd" shows that this node has become the primary. We can also see the syncing process

![x](/static/2023-10-13-drbd/09.png)

<br>

The sync process can also bee seen on the secondary node

![x](/static/2023-10-13-drbd/09a.png)

<br>

After some minutes, the sync process should complete

> node1

![x](/static/2023-10-13-drbd/10.png)

> node2

![x](/static/2023-10-13-drbd/10a.png)

<br>

We can also run 'drbdadm status' to see the status

> node1

![x](/static/2023-10-13-drbd/11.png)

> node2

![x](/static/2023-10-13-drbd/11a.png)

<br>

Next, create an ext4 file system on the DRBD block device

```shell
mkfs.ext4 /dev/drbd0 
```

![x](/static/2023-10-13-drbd/12.png)

> * Ext4 is a widely used and robust file system format for Linux

<br>
<br>

## Mounting the DRBD Device

Now on the node1, create a directory to mount the device and mount the device

```shell
mkdir /opt/helenadrbd
mount /dev/drbd0 /opt/helenadrbd
```

<br>

Create a test file with this command

```shell
cd /opt/helenadrbd
touch test1.txt
```

![x](/static/2023-10-13-drbd/13.png)

<br>

Now let's dismount the device from node1 and try mouting it on node2.

```shell
umount /opt/helenadrbd
```

<br>

Set the node1 to become secondary node

```shell
drbdadm secondary r0
```

![x](/static/2023-10-13-drbd/14.png)

<br>

Move over to the node2, set this as the primary node

```shell
drbdadm primary r0
```

![x](/static/2023-10-13-drbd/15.png)

<br>

And mount the DRBD device

```shell
mkdir /opt/helenadrbd
mount /dev/drbd0 /opt/helenadrbd
```

![x](/static/2023-10-13-drbd/16.png)

As we can see, the file inside the device is replicated and still show up intact on the second node.

<br>
<br>

## Pacemaker

> Pacemaker is a cluster resource manager and high-availability solution that can be used in conjunction with DRBD to create and manage highly available clusters. It provides automated failover and resource management capabilities for services and resources that use DRBD for data replication.

Pacemaker will help us automate the failover process that we had to do manually earlier, to use peacemaker first disable the service process of drbd

![x](/static/2023-10-13-drbd/17.png)

<br>

Then install pacemaker with these commands

```shell
apt install pacemaker pcs -y
apt install resource-agents -y
```

<br>

Configure the corosync conf file

> Corosync ensures that all nodes in the cluster can communicate and synchronize their activities, making it a foundational component for Pacemaker to function effectively in a cluster environment.

```shell
nano /etc/corosync/corosync.conf
```

```conf
# Cluster communication settings
totem {
    version: 2          # Corosync protocol version
    secauth: off        # Security authentication is disabled
    cluster_name: helenacluster   # Cluster name
    transport: udpu     # UDP unicast transport
}

# Node configuration
nodelist {
    node {
        ring0_addr: 198.18.0.31  # First node's IP address
        name: storage1           # First node's name
    }
    node {
        ring0_addr: 198.18.0.32  # Second node's IP address
        name: storage2           # Second node's name
    }
}

# Quorum settings
quorum {
    provider: corosync_votequorum   # Quorum provider
    two_node: 1                     # Allow quorum with just two nodes
}
```

<br>

Restart the services for the config to take effect

```shell
systemctl restart corosync
systemctl restart pacemaker
```

<br>

Repeat the process for the second node, and after that run 'crm status' to see the pacemaker status

![x](/static/2023-10-13-drbd/18.png)

<br>

From now on, we configure the pacemaker cluster configuration only from the primary node.

The first ones are to disable quorum policy and STONITH, a mechanism for isolating failed nodes in the cluster

```shell
pcs property set no-quorum-policy=ignore
pcs property set stonith-enabled=false
```

<br>

Next we create the resource for the HA

```shell
pcs cluster cib drbdconf  
```

<br>

Next create a Pacemaker resource named "p_drbd_r0" to manage a DRBD resource "r0." This sets parameters for starting, stopping, and monitoring the resource, including timeout values for various operations.

```shell
pcs -f drbdconf resource create p_drbd_r0 ocf:linbit:drbd \
drbd_resource=r0 \
op start interval=0s timeout=240s \
stop interval=0s timeout=100s \
monitor interval=31s timeout=20s \
role=Unpromoted monitor interval=29s timeout=20s role=Promoted
```

<br>

Next run this command to configure the "p_drbd_r0" resource to be promotable in the Pacemaker cluster. It specifies constraints for promotion, clone, and notifications.

```shell
pcs -f drbdconf resource promotable p_drbd_r0 \
promoted-max=1 promoted-node-max=1 clone-max=2 clone-node-max=1 notify=true
```

<br>

Then push the config

```shell
pcs cluster cib-push drbdconf
```

![x](/static/2023-10-13-drbd/19.png)

<br>

Next, create a Pacemaker resource named "p_fs_drbd0" to manage a filesystem on a DRBD device (/dev/drbd0). The filesystem is mounted at the "/opt/helenadrbd" directory, using the ext4 filesystem type with specific options. It sets parameters for starting, stopping, and monitoring the resource.

```shell
pcs -f drbdconf resource create p_fs_drbd0 ocf:heartbeat:Filesystem \
device=/dev/drbd0 directory=/opt/helenadrbd fstype=ext4 \
options=noatime,nodiratime \
op start interval="0" timeout="60s" \
stop interval="0" timeout="60s" \
monitor interval="20" timeout="40s" 
```

<br>

Then run these commands to define constraints for resource ordering and colocation in the Pacemaker cluster. They ensure that the "p_drbd_r0" resource is promoted before starting the "p_fs_drbd0" resource, and that they are colocated with the "Promoted" role.

```shell
pcs -f drbdconf constraint order promote p_drbd_r0-clone then start p_fs_drbd0
pcs -f drbdconf constraint colocation add p_fs_drbd0 with p_drbd_r0-clone INFINITY with-rsc-role=Promoted
```

<br>

Lastly, push the configuration

```shell
pcs cluster cib-push drbdconf
```

![x](/static/2023-10-13-drbd/20.png)

<br>

Run 'crm status' to see the pacemaker status

![x](/static/2023-10-13-drbd/21.png)

> This cluster summary provides an overview of a Pacemaker cluster with two nodes ("storage1" and "storage2"). It is currently operational with quorum and manages several resources, including a promotable DRBD clone set and a filesystem resource. The current designated controller is "storage1" with a summary showing that the cluster is in a healthy and operational state.

<br>
<br>

## Testing Pacemaker's Auto Failover

Right now both nodes are up and node1 being the primary where the DRBD device is mounted on

![x](/static/2023-10-13-drbd/22.png)

<br>

Now we're gonna simulate a node failure by running command "echo b > /proc/sysrq-trigger". This will make the node1 immediately reboots

```shell
echo b > /proc/sysrq-trigger
```

<br>

And while the node1 is down, the node2 starts taking over as the primary node and automatically mount the DRBD device

![x](/static/2023-10-13-drbd/23.png)

<br>




